{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display_html\n",
    "import numpy as np\n",
    "\n",
    "# Source of code: https://stackoverflow.com/questions/28763891/what-should-i-do-when-tr-has-rowspan\n",
    "import html_table_to_pandas as html_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football data from wikipedia\n",
    "\n",
    "The objective of this notebook is to extract information on British football teams from Wikipedia. This primarily utilises the wikipedia API to extract the data, then Beautiful soup is used to manipulate the data.\n",
    "\n",
    "## Wikipedia API\n",
    "\n",
    "The documentation for the wikipedia API is here:\n",
    "https://www.mediawiki.org/wiki/API:Main_page\n",
    "\n",
    "The inputs to the get request are:\n",
    "- URL: the wikipedia API endpoint used throughout.\n",
    "- TITLE: the title of the page to be scraped\n",
    "- PARAMS: the API takes its input through parameters provided by the query string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session() # This object allows you to persist certain parameters across requests. Results in better performance.\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\" # English wikipedia API endpoint\n",
    "\n",
    "TITLE = \"List of football clubs in England\"\n",
    "\n",
    "PARAMS = {\n",
    "    'action': 'parse', # the action is to parse the content of the page\n",
    "    'page': TITLE,\n",
    "    'format': 'json'   # the API can return other formats but they are standardising to JSON\n",
    "    #'UTF8':            # convert to UTF-8 encoding - does this work? Is it already UTF-8?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting data from a page\n",
    "Next, we must request the data from the wiki page and turn it into a python object that we can easily manipulate. We must interpret the request data as JSON in order to convert it into __class 'dict'__.\n",
    "\n",
    "The scraped data are also stored in a JSON file for future use so that we do not need to make the same request multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Get the response data as a python object. \n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json() # Interpret the request data as json: print(type(DATA)) --> <class 'dict'>\n",
    "\n",
    "# Write to JSON file\n",
    "with open('list_clubs_in_england.json', 'w') as f:\n",
    "    json.dump(DATA, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "with open('list_clubs_in_england.json') as data_file:\n",
    "    DATA = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data structure within the dictionary object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse=DATA['parse']\n",
    "parse.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = parse['text']\n",
    "type(text['*'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating the html\n",
    "\n",
    "#### Defining the function(s)\n",
    "Here is the original code where the following functions are defined: https://stackoverflow.com/questions/28763891/what-should-i-do-when-tr-has-rowspan\n",
    "\n",
    "Very useful when a table has rowspan and colspan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producing the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the HTML using Beautiful Soup\n",
    "soup = BeautifulSoup(DATA['parse']['text']['*'], \"html.parser\")\n",
    "# print(soup.prettify())\n",
    "\n",
    "# Extract all of the tables from the soup and put them into separate elements of a list\n",
    "alltables = soup.findAll('table')\n",
    "print(\"Number of tables found : \" , len(alltables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltab_parse = [html_pd.main(alltables[tab]) for tab in range(1, 25)]\n",
    "\n",
    "result = pd.concat(alltab_parse)\n",
    "result = result.drop(result.loc[result.index == 0]).reset_index(drop=True)\n",
    "result.columns = ['Club', 'League/Division', 'Level','Nickname','Change 2017-2018']\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 20 teams in the Premier League\n",
    "prem_teams = result.loc[result['League/Division'] == 'Premier League']\n",
    "prem_teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract team information on Premier League"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Extract data on all Premier League teams, convert from JSON to dictionary structure. Store each dictionary structure in a list\n",
    "club_data_json = [(S.get(url=URL, params={'action': \"parse\",'page': row.Club,'format': \"json\"})).json() for row in prem_teams.itertuples()]\n",
    "\n",
    "# Write to a file. Each teams data is written on a single line\n",
    "with open('premier_league_club_data.json', 'w') as f:\n",
    "    json.dump(club_data_json, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('premier_league_club_data.json') as f:\n",
    "    club_data_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the first team page data into bs4\n",
    "soup_club1 = BeautifulSoup(club_data_json[0]['parse']['text']['*'], \"html.parser\")\n",
    "\n",
    "alltables_club1 = soup_club1.findAll(\"table\")\n",
    "\n",
    "html_content = str(alltables_club1[2])\n",
    "display_html(html_content, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_html(html_content)\n",
    "dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_club1 = pd.concat([dfs[1], dfs[2]])\n",
    "players_club1 = players_club1.drop(players_club1.loc[result.index == 0]).reset_index(drop=True)\n",
    "players_club1 = players_club1.drop(columns=1)\n",
    "players_club1.columns = ['No.', 'Position', 'Player']\n",
    "\n",
    "# Remove link references and roles\n",
    "players_club1['Player'] = players_club1['Player'].str.extract(r\"^(\\w+\\s\\w+)\", expand = False)\n",
    "\n",
    "players_club1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Club official info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must check the number of columns\n",
    "club1_officials=parse_html_table(alltables_club1[7], alltables_club1[7])\n",
    "club1_officials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player data\n",
    "\n",
    "Retrieve data on each individual player.\n",
    "\n",
    "What kind of data and what format?\n",
    "\n",
    "### Check for disambiguation page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_PLAYER = 'Simon Francis'\n",
    "\n",
    "PARAMS_DISAMB = {\n",
    "    'action': 'query', # the action is to parse the content of the page\n",
    "    'titles': TITLE_PLAYER,\n",
    "    'format': 'json', # the API can return other formats but they are standardising to JSON\n",
    "    'prop': 'categories'\n",
    "}\n",
    "\n",
    "# Get the response data as a python object. \n",
    "R = S.get(url=URL, params=PARAMS_DISAMB)\n",
    "player_1_query = R.json() # Interpret the request data as json: print(type(DATA)) --> <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_disambiguation(player_1_query, TITLE_PLAYER):\n",
    "    #  Convert to string in order to easily test if contains specific string\n",
    "    player_1_query = json.dumps(player_1_query)\n",
    "\n",
    "    if 'Category:All disambiguation pages' in player_1_query:\n",
    "        TITLE_PLAYER = TITLE_PLAYER + ' (footballer)'\n",
    "    return TITLE_PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_PLAYER = f_disambiguation(player_1_query, TITLE_PLAYER)\n",
    "TITLE_PLAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve player info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PARAMS_PLAYER = {\n",
    "    'action': 'parse', # the action is to parse the content of the page\n",
    "    'page': TITLE_PLAYER,\n",
    "    'format': 'json' # the API can return other formats but they are standardising to JSON\n",
    "}\n",
    "\n",
    "\n",
    "# Get the response data as a python object. \n",
    "R = S.get(url=URL, params=PARAMS_PLAYER)\n",
    "DATA_PLAYER = R.json() # Interpret the request data as json: print(type(DATA)) --> <class 'dict'>\n",
    "\n",
    "# Write to JSON file\n",
    "with open('player_data.json', 'w') as f:\n",
    "    json.dump(DATA_PLAYER, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "with open('player_data.json') as data_file:\n",
    "    DATA_PLAYER = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the first team page data into bs4\n",
    "soup_player1 = BeautifulSoup(DATA_PLAYER['parse']['text']['*'], \"html.parser\")\n",
    "\n",
    "alltables_player1 = soup_player1.findAll(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = alltables_player1[1]\n",
    "## run the above functions to extract the data\n",
    "rows, num_rows, num_cols = pre_process_table(table)\n",
    "df = process_rows(rows, num_rows, num_cols)\n",
    "\n",
    "# Re-do the formatting\n",
    "df = df[~df[1].isin(['Total', 'Career total', 'Season'])] # Remove rows containing totals and headers\n",
    "df = df.drop([11, 12], axis = 1) # Remove columns containing totals\n",
    "\n",
    "# Re-do the headers\n",
    "player_cols = ['Club', 'Season','Division', 'League - Apps', 'League - Goals', 'FA Cup - Apps', 'FA Cup - Goals', 'League Cup - Apps', 'League Cup - Goals', 'Other - Apps', 'Other - Goals']\n",
    "df.columns = player_cols # Apply the correct headers\n",
    "\n",
    "# Remove link references from 'Season' column\n",
    "df['Season'] = df['Season'].str.extract(r\"^(\\d{4}â€“\\d{2})\", expand = False)\n",
    "\n",
    "# TO DO: convert data type of Season column to date/year\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multiple players\n",
    "Retrieve player stats given a list of player names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DATA_PLAYERS_TEAM = []\n",
    "\n",
    "for index in players_club1.index:\n",
    "    PLAYER = players_club1.at[index,'Player']\n",
    "    PARAMS_PLAYER = {\n",
    "    'action': 'parse', # the action is to parse the content of the page\n",
    "    'page': PLAYER,\n",
    "    'format': 'json' # the API can return other formats but they are standardising to JSON\n",
    "    }\n",
    "    \n",
    "    # Get the response data as a python object. \n",
    "    R = S.get(url=URL, params=PARAMS_PLAYER)\n",
    "    DATA_PLAYER = R.json() # Interpret the request data as json: print(type(DATA)) --> <class 'dict'>\n",
    "    TITLE_PLAYER = f_disambiguation(DATA_PLAYER, PLAYER)\n",
    "    \n",
    "    if (TITLE_PLAYER == PLAYER):\n",
    "        DATA_PLAYERS_TEAM.append(DATA_PLAYER)\n",
    "    else:\n",
    "        PARAMS_PLAYER = {\n",
    "        'action': 'parse', # the action is to parse the content of the page\n",
    "        'page': TITLE_PLAYER,\n",
    "        'format': 'json' # the API can return other formats but they are standardising to JSON\n",
    "        }\n",
    "\n",
    "        # Get the response data as a python object. \n",
    "        R = S.get(url=URL, params=PARAMS_PLAYER)\n",
    "        DATA_PLAYER = R.json() # Interpret the request data as json: print(type(DATA)) --> <class 'dict'>\n",
    "        DATA_PLAYERS_TEAM.append(DATA_PLAYER)\n",
    "\n",
    "# Write to JSON file\n",
    "with open('players_data_test.json', 'w') as f:\n",
    "    json.dump(DATA_PLAYERS_TEAM, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "with open('players_data_test.json') as data_file:\n",
    "    DATA_PLAYER = json.load(data_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
